{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadbc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e86729",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "\n",
    "\n",
    "def get_usernames(doc,details_dict):\n",
    "    us = doc.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "    for tag in us:\n",
    "        details_dict['Username'].append(tag.text.strip())\n",
    "    return\n",
    "\n",
    "def get_personal_info(doc,details_dict):\n",
    "    pi = doc.find_all('div',{'class':'col-md-7'})\n",
    "    for tag in pi:\n",
    "        details_dict['Info'].append(tag.text.strip())\n",
    "    return   \n",
    "\n",
    "#######################################################################\n",
    "def parse_count(count):\n",
    "    if(count[-1]=='k'):\n",
    "        return int(float(count[:-1])*1000)\n",
    "    else:\n",
    "        return int(count)\n",
    "\n",
    "\n",
    "\n",
    "def get_full_name(doc):\n",
    "    fn = doc.find_all('span',{'class':'p-name vcard-fullname d-block overflow-hidden'})\n",
    "    if(fn == []):\n",
    "        fn_org = doc.find_all('h1',{'class':'h2 lh-condensed'})\n",
    "        status = 1\n",
    "        return fn_org[0].text.strip(),status\n",
    "    status = 0 \n",
    "    return fn[0].text.strip(),status\n",
    "\n",
    "\n",
    "def get_followers(doc,status):\n",
    "    if status==1:\n",
    "        return '-'\n",
    "    fl1 = doc.find_all('div',{'class':'mb-3'})\n",
    "    fl2 = fl1[3].find_all('a',{'class':'Link--secondary no-underline no-wrap'})\n",
    "    fl3 = fl2[0].find('span',{'class':'text-bold color-fg-default'}).text\n",
    "    return(parse_count(fl3))\n",
    "\n",
    "def get_following(doc,status):\n",
    "    if status==1:\n",
    "        return '-'\n",
    "    fw1 = doc.find_all('div',{'class':'mb-3'})\n",
    "    fw2 = fw1[3].find_all('a',{'class':'Link--secondary no-underline no-wrap'})\n",
    "    fw3 = fw2[1].find('span',{'class':'text-bold color-fg-default'}).text\n",
    "    return(parse_count(fw3))\n",
    "\n",
    "def get_no_of_stars(doc,status):\n",
    "    if status==1:\n",
    "        return '-'\n",
    "    fs1 = doc.find_all('div',{'class':'mb-3'})\n",
    "    fs2 = fs1[3].find_all('a',{'class':'Link--secondary no-underline no-wrap'})\n",
    "    fs3 = fs2[2].find('span',{'class':'text-bold color-fg-default'}).text\n",
    "    return(parse_count(fs3))\n",
    "    \n",
    "def get_location(doc,status):\n",
    "    if status==1:\n",
    "        l = doc.find_all('span',{'itemprop':'location'})\n",
    "        if(l==[]):\n",
    "            return '-'\n",
    "        return(l[0].text)\n",
    "    loc = doc.find_all('span',{'class':'p-label'})\n",
    "    if(loc==[]):\n",
    "        return '-'\n",
    "    return(loc[0].text.strip())\n",
    "\n",
    "def get_no_of_repos(doc,status): \n",
    "    if status==1:\n",
    "        return '-'\n",
    "    repono = doc.find_all('span',{'class':'Counter'})\n",
    "    return(parse_count(repono[0].text))\n",
    "\n",
    "def get_no_of_con(doc,status):\n",
    "    if status==1:\n",
    "        return '-'\n",
    "    conno = doc.find_all('h2',{'class':'f4 text-normal mb-2'})\n",
    "    return(conno[0].text.split()[0])\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c8655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detail_scrapper(url,details_dict):#beta\n",
    "    response = requests.get(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    \n",
    "    \n",
    "    det = (get_full_name(doc))\n",
    "    if(det[1] == 1):\n",
    "        status = 1\n",
    "    else:\n",
    "        status = 0\n",
    "      \n",
    "    \n",
    "    details_dict['Full Name'].append(det[0])    \n",
    "    details_dict['Followers'].append(get_followers(doc,status))\n",
    "    details_dict['Following'].append(get_following(doc,status))\n",
    "    details_dict['No. of stars'].append(get_no_of_stars(doc,status))\n",
    "    details_dict['Location'].append(get_location(doc,status))\n",
    "    details_dict['Repositories'].append(get_no_of_repos(doc,status))\n",
    "    details_dict['Contributions (Last Year)'].append(get_no_of_con(doc,status))\n",
    "    return\n",
    "    \n",
    "\n",
    "def diver(doc,details_dict):#baap\n",
    "\n",
    "    dive = doc.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "    for i in range(len(dive)):\n",
    "        dive_url = 'https://github.com' + dive[i].find('a')['href']\n",
    "        print(\"Collecting data about {}...\".format(dive_url.split('/')[-1]))\n",
    "        detail_scrapper(dive_url,details_dict)\n",
    "\n",
    "    df = pd.DataFrame(details_dict)\n",
    "    path = 'Github_Sponsors/{}.csv'.format('Github Sponsors Data')\n",
    "    df.to_csv(path,index = None)\n",
    "    print('Process Completed Successfully.')\n",
    "    return \n",
    "   \n",
    "def front():#sabkabaap\n",
    "    sponsors_url = 'https://github.com/sponsors/community'\n",
    "    response = requests.get(sponsors_url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(sponsors_url))\n",
    "\n",
    "    sponsor_details_dict = {\n",
    "    'Username':[],'Info':[],'Full Name':[],'Followers':[],'Following': [],'No. of stars': [],'Location':[],'Repositories':[],'Contributions (Last Year)': []\n",
    "    }\n",
    "    os.makedirs('Github_Sponsors',exist_ok = True)\n",
    "    get_usernames(doc,sponsor_details_dict)\n",
    "    get_personal_info(doc,sponsor_details_dict)\n",
    "    diver(doc,sponsor_details_dict)\n",
    "    return doc\n",
    "       \n",
    "        \n",
    "#################################\n",
    "def individual_repos(url):\n",
    "    response = requests.get(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    \n",
    "    user_repo_dict = {'Repo Name':[],'URL':[]}    \n",
    "    #status 1\n",
    "    repname = doc.find_all('h3',{'class':'wb-break-all'})\n",
    "    for i in range(len(repname)):\n",
    "        user_repo_dict['URL'].append('https://github.com' + repname[i].find('a')['href'])\n",
    "        user_repo_dict['Repo Name'].append(repname[i].find('a').text.strip())\n",
    "    \n",
    "    if(url.split('/')[-1]!='repositories'):\n",
    "        pname = url.split('/')[-1].split('?')[0]\n",
    "    else:\n",
    "        pname = url.split('/')[-2]\n",
    "    path = 'Github_Sponsors/{}.csv'.format(pname)\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping....\".format(path))\n",
    "        return\n",
    "              \n",
    "\n",
    "    pd.DataFrame(user_repo_dict).to_csv(path,index=None)\n",
    "    return\n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "def individual(doc):\n",
    "    dive = doc.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "    for i in range(len(dive)):\n",
    "        dive_url = 'https://github.com' + dive[i].find('a')['href']    \n",
    "        response = requests.get(dive_url)\n",
    "        doc = BeautifulSoup(response.text, 'html.parser')\n",
    "        if response.status_code != 200:\n",
    "            raise Exception('Failed to load page {}'.format(dive_url))\n",
    "        \n",
    "        print(\"Extracting data about {}'s repositories\".format(dive_url.split('/')[-1]))\n",
    "        det = (get_full_name(doc))\n",
    "        if(det[1] == 1):\n",
    "            status = 1\n",
    "        else:\n",
    "            status = 0\n",
    "        \n",
    "        if status == 0:\n",
    "            repo_link = doc.find_all('div',{'class':'UnderlineNav width-full box-shadow-none'})\n",
    "            rep = repo_link[0].find_all('nav',{'class':'UnderlineNav-body'})[0].find_all('a',{'class':'UnderlineNav-item'})[1]['href']\n",
    "         \n",
    "        else: \n",
    "            repo_link = doc.find_all('div',{'class':'width-full d-flex position-relative container-xl'})\n",
    "            rep = repo_link[0].find_all('li',{'class':'d-flex js-responsive-underlinenav-item'})[1].find_all('a',{'class':'UnderlineNav-item'})[0]['href']\n",
    "       \n",
    "\n",
    "        url = 'https://github.com' + rep\n",
    "        individual_repos(url)\n",
    "    print('Process Completed Successfully.')\n",
    "    return\n",
    " \n",
    "def extract():\n",
    "    individual(front())\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffb70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data about patricia-gallardo...\n",
      "Collecting data about claui...\n",
      "Collecting data about curl...\n",
      "Collecting data about eslint...\n",
      "Collecting data about EbookFoundation...\n",
      "Collecting data about potatoqualitee...\n",
      "Collecting data about M0nica...\n",
      "Collecting data about chaynHQ...\n",
      "Collecting data about CommandPost...\n",
      "Collecting data about aurelia...\n",
      "Collecting data about aeneasr...\n",
      "Collecting data about phalcon...\n",
      "Collecting data about nzakas...\n",
      "Collecting data about flarum...\n",
      "Collecting data about sanderstad...\n",
      "Collecting data about tenancy...\n",
      "Collecting data about katef...\n",
      "Collecting data about joshuaulrich...\n",
      "Collecting data about shentao...\n",
      "Collecting data about wifelette...\n",
      "Collecting data about jina...\n",
      "Collecting data about directus...\n",
      "Collecting data about snipe...\n",
      "Collecting data about freakboy3742...\n",
      "Collecting data about richfelker...\n",
      "Process Completed Successfully.\n",
      "Extracting data about patricia-gallardo's repositories\n",
      "Extracting data about claui's repositories\n",
      "Extracting data about curl's repositories\n",
      "Extracting data about eslint's repositories\n",
      "Extracting data about EbookFoundation's repositories\n",
      "Extracting data about potatoqualitee's repositories\n",
      "Extracting data about M0nica's repositories\n",
      "Extracting data about chaynHQ's repositories\n",
      "Extracting data about CommandPost's repositories\n",
      "Extracting data about aurelia's repositories\n",
      "Extracting data about aeneasr's repositories\n",
      "Extracting data about phalcon's repositories\n",
      "Extracting data about nzakas's repositories\n",
      "Extracting data about flarum's repositories\n",
      "Extracting data about sanderstad's repositories\n",
      "Extracting data about tenancy's repositories\n",
      "Extracting data about katef's repositories\n",
      "Extracting data about joshuaulrich's repositories\n",
      "Extracting data about shentao's repositories\n",
      "Extracting data about wifelette's repositories\n",
      "Extracting data about jina's repositories\n",
      "Extracting data about directus's repositories\n",
      "Extracting data about snipe's repositories\n",
      "Extracting data about freakboy3742's repositories\n",
      "Extracting data about richfelker's repositories\n",
      "Process Completed Successfully.\n"
     ]
    }
   ],
   "source": [
    "extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b3fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e83d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d4501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f56299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
